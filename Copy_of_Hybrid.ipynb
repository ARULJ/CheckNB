{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ij-UkMXFukBuXKbKb8krvENM4b9Bihnp",
      "authorship_tag": "ABX9TyMPkDFQ7Mux14P5U7+yFWQU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j-s34vl17ud",
        "outputId": "031c59ac-8eb5-4836-aa58-531d2e72d1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: surprise in /usr/local/lib/python3.10/dist-packages (0.1)\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.10/dist-packages (from surprise) (1.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG_2KJ5ho_rX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from surprise import Dataset, Reader, SVD, KNNBasic, accuracy\n",
        "from surprise.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load the datasets\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/MovieLensDataset/rating.csv')\n",
        "movies = pd.read_csv('/content/drive/MyDrive/MovieLensDataset/movie.csv')\n",
        "\n",
        "# Prepare the data for Surprise (Collaborative Filtering)\n",
        "reader = Reader(rating_scale=(0.5, 5.0))\n",
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning for SVD\n",
        "param_grid_svd = {\n",
        "    'n_factors': [50, 100, 150],\n",
        "    'n_epochs': [20, 30, 40],\n",
        "    'lr_all': [0.002, 0.005, 0.01],\n",
        "    'reg_all': [0.02, 0.05, 0.1]\n",
        "}\n",
        "#gs_svd = GridSearchCV(SVD, param_grid_svd, measures=['rmse'], cv=3)\n",
        "#gs_svd.fit(data)\n",
        "\n",
        "#best_svd = gs_svd.best_estimator['rmse']\n",
        "#best_svd.fit(trainset)\n",
        "\n",
        "svd = SVD()\n",
        "svd.fit(trainset)\n",
        "\n",
        "# Preprocess the data for content-based filtering\n",
        "movies['genres'] = movies['genres'].fillna('')\n",
        "movies['combined_features'] = movies['genres']\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(movies['combined_features'])\n",
        "\n",
        "# Compute the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Function to get collaborative filtering recommendations\n",
        "def get_collaborative_recommendations(user_id, n=10):\n",
        "    movie_ids = ratings['movieId'].unique()\n",
        "    predictions = [svd.predict(user_id, str(movie_id)) for movie_id in movie_ids]\n",
        "    recommendations = sorted(predictions, key=lambda x: x.est, reverse=True)[:n]\n",
        "    recommended_movie_ids = [int(pred.iid) for pred in recommendations]\n",
        "    recommended_movies = movies[movies['movieId'].isin(recommended_movie_ids)]\n",
        "    return recommended_movies\n",
        "\n",
        "# Function to get content-based recommendations\n",
        "def get_content_based_recommendations(title, n=10):\n",
        "    idx = movies[movies['title'] == title].index[0]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:n+1]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    return movies.iloc[movie_indices]\n",
        "\n",
        "# Function to combine recommendations\n",
        "def get_hybrid_recommendations(user_id, title, n=10):\n",
        "    collaborative_recs = get_collaborative_recommendations(user_id, n)\n",
        "    content_recs = get_content_based_recommendations(title, n)\n",
        "\n",
        "    # Combine both recommendations\n",
        "    hybrid_recs = pd.concat([collaborative_recs, content_recs]).drop_duplicates().head(n)\n",
        "    return hybrid_recs\n",
        "\n",
        "# Example usage\n",
        "user_id = 1\n",
        "title = 'Toy Story (1995)'\n",
        "\n",
        "print(f\"Top 10 hybrid recommendations for User ID {user_id} and movie '{title}':\")\n",
        "print(get_hybrid_recommendations(user_id, title))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "btY8b84ZpK6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}